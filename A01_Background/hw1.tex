\documentclass[a4paper]{article}
\usepackage[top=.75in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{Sweave}

\title{\vspace{-30pt}STAT 745 -- Fall 2014\\Assignment 1}
\author{Group 1: Francene Cicia, Lonie Moore, Doug Raffle, Melissa Smith}
\date{August 25, 2014}

\renewcommand{\thesubsection}{\thesection.\alph{subsection}}

\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
\setlength{\parindent}{0pt}
\vspace{-50pt}
\maketitle

\section{Artificial Intelligence vs. Machine Learning}

\subsection{Identify and describe $\mathcal{G}, y, x, A$}
\begin{itemize}
  \item $\mathcal{G}$ is the set of possible response classes.  In
    this case, there are
    $k=|\mathcal{G}|=2$ levels:
    Artificial Intelligence (AI) and
    Machine Learning (ML) (i.e., $\mathcal{G} = \left\{\text{AI}, \:\text{ML} \right\}$).
  \item $y$ is the vector of responses for the
    $n = 879$ observed papers ($y_i \in \mathcal{G}$).
  \item $x$ is a $879\times141$ matrix
    showing the count of the tokens in the title of every paper (where
    $p=141$ is the number of unique tokens in the corpus).
    For every row $x_i$, $x_{ij}$ is the frequency of token $j$ in
    paper $i$'s title.
  \item $A$ is a $879\times879$ symmetrical matrix which
    represents a graph of the papers' shared citations.  Each
    $A_{ij}$ is the number of citations that papers $i$ and $j$ share.  The diagonals,
    $A_{ii}$, are the number of citations for paper $i$.
\end{itemize}

\subsection{Strategies}
If we were to view this as a supervised problem, we would treat the topic ($y$) as
a response and train a model to classify papers based on
the words in the title and/or the citation network, using a technique like $K$-Nearest Neighbors.\\

In an unsupervised context, we would ignore the class (or
treat it as another feature) and use some form of Cluster
Analysis to see how similar the observations are to each other, though
there is no guarantee that the papers would cluster by topic.

\subsection{The First Paper}
The tokens in the title are:
\begin{Schunk}
\begin{Sinput}
> inds <- which(tokens[1,] > 0)
> cat(names(tokens[1,inds]), "\n", sep="\t")
\end{Sinput}
\begin{Soutput}
experiment	induction	plan	with	
\end{Soutput}
\end{Schunk}
Since the titles were tokenized as lexemes (i.e., word roots), not
whole words, and most function words (e.g., prepositions, articles, etc.) have
been discarded, there is obviously some
guesswork.  The title may be something like ``Planning Experiments with
Induction,'' ``Experimental Planning with Induction,'' or ``Induction
with Experimental Planning.''  It's impossible to be at all certain
without inflectional prefixes or suffixes, function words, or more
context.\\

A simple method for finding the papers most similar to the first
would be to treat the papers as points in $p$-space, and selecting the
papers that are the closest to the first using Euclidean Distances.\\

\begin{Schunk}
\begin{Sinput}
> library(parallel); cl <- makeCluster(detectCores()); clusterExport(cl, "tokens")
> dis <- parRapply(cl, tokens[-1,], function(r) sqrt(sum((tokens[1,]-r)^2)))
> stopCluster(cl)
> (nearest.tok <- as.integer(names(head((sort(dis))))))
\end{Sinput}
\begin{Soutput}
[1]  37 116 120 217 368 538
\end{Soutput}
\end{Schunk}

We would hope, then, that these papers share the same topic, at least on average,
as the first ($y_1=$AI):
\begin{Schunk}
\begin{Sinput}
> as.character(class[nearest.tok])
\end{Sinput}
\begin{Soutput}
[1] "ML" "AI" "AI" "AI" "AI" "ML"
\end{Soutput}
\end{Schunk}
This method, however,  only considers the token features.  To find similar papers in terms of
the citation graph, we need only find the papers (nodes) that have
highest number of shared citations (edge values).
\begin{Schunk}
\begin{Sinput}
> names(cite) <- gsub("node_", "", names(cite))
> (nearest.g <- as.integer(names(head(sort(unlist(cite[1, -1]), decreasing=TRUE)))))
\end{Sinput}
\begin{Soutput}
[1]   2   3 574 360 576 626
\end{Soutput}
\begin{Sinput}
> as.character(class[nearest.g])
\end{Sinput}
\begin{Soutput}
[1] "AI" "AI" "AI" "AI" "AI" "AI"
\end{Soutput}
\end{Schunk}
The graph based approach seems to outperform the tokenized titles in selecting papers
with the same topic.

\section{Absolute Error Loss}
\begin{align*}
  \hat{f}(x) &= \argmin_c \: \text{E}_{Y|X}\left[\lvert Y - c \rvert \:\middle|\: X=x\right]\\
\end{align*}

We begin by re-writing the expectation:
\begin{align*}
  \text{E}_{Y|X}\left[\lvert Y - c \rvert \:\middle|\: X=x\right] =
  \int_{-\infty}^{\infty} \lvert y - c \rvert \: f_x(y) \: dy
  &= \int_{-\infty}^{c} \left(c - y \right) \: f_x(y) \: dy +
     \int_{c}^{\infty} \left(y - c \right) \: f_x(y) \: dy\\
\end{align*}
Then we take the derivative using Leibniz's Rule:
\begin{align*}
  \frac{\partial}{\partial c} \int_{-\infty}^{c} \left(c - y \right) \: f_x(y) \: dy +
  \frac{\partial}{\partial c} \int_{c}^{\infty} \left(y - c \right) \: f_x(y) \: dy &=
  \int_{-\infty}^{c} \frac{\partial}{\partial c} \left(c - y \right) \: f_x(y) \: dy +
  \int_{c}^{\infty} \frac{\partial}{\partial c} \left(y - c \right) \: f_x(y) \: dy\\
  &= \int_{-\infty}^{c} f_x(y) \: dy - \int_{c}^{\infty} f_x(y) dy\\
\end{align*}
Which we set to zero and solve for $c$ to find $\argmin_c$:
\begin{align*}
  \int_{-\infty}^{c} f_x(y) \: dy - \int_{c}^{\infty} f_x(y) dy &= 0\\
  \int_{-\infty}^{c} f_x(y) \: dy &= \int_{c}^{\infty} f_x(y) dy\\
  Pr\left(Y \le c \;\middle|\; X=x\right) &= Pr\left(Y > c \;\middle|\; X=x\right)\\
  Pr\left(Y \le c \;\middle|\; X=x\right) &= 1 - Pr\left(Y \le c \;\middle|\; X=x\right)\\
  2Pr\left(Y \le c \;\middle|\; X=x\right) &= 1\\
  Pr\left(Y \le c \;\middle|\; X=x\right) &= \frac{1}{2}\\
\end{align*}
So $\argmin_c$ is, by definition, the conditional median of $Y$ given $X=x$, leaving us with:
\begin{align*}
  \hat{f}(x) = \argmin_c \: \text{E}_{Y|X}\left[\lvert Y - c \rvert \:\middle|\: X=x\right]
  = \text{median}\left(Y \;\middle|\; X=x\right)\\
\end{align*}

Which tells us that using $L_1$ loss for $K$-NN is to quantile regression what using $L_2$ loss is
to least squares regression.


\end{document}
